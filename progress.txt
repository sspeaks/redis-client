# Ralph Progress Log
Started: Wed Feb 11 14:27:03 PST 2026

## Codebase Patterns
- Build C# benchmarks with `dotnet build` in `benchmarks/dotnet/RedisBenchmark/`
- The redis-client executable uses `AppConfig.RunState` for all CLI options, parsed via `System.Console.GetOpt`
- `MultiplexPool.createMultiplexPool` takes a `Connector` and `Int` (mux count per node); use 1 for default, higher values only help with high thread counts on real network latency
- Optimal bench config on localhost: 16 threads, 1 mux per node; above 16 threads, contention degrades throughput
- Existing bench mode in Main.hs reuses `-d` flag for ops count and `-n` for thread count
- Always add `.gitignore` with `bin/` and `obj/` for dotnet projects to avoid committing build artifacts
- TLS vs Plaintext connectors are different types — can't mix in `if-then-else`; use separate code branches
- Use `encodeCommandBuilder` from `RedisCommandClient` to build RESP Builders for `MultiplexPool.submitToNode`
- For SET/GET hot paths, use specialized `encodeSetBuilder`/`encodeGetBuilder` instead of `encodeCommandBuilder` — avoids list allocation and length traversal
- Use NOINLINE on constant Builders (preambles) so they're computed once as CAFs; use INLINE on encoder functions for call-site specialization
- `ClusterClient(..)` is in `ClusterCommandClient` module; access `clusterTopology` to read topology TVar
- Pre-existing nix-build failure in `test/ClusterE2E/Utils.hs` — not caused by recent changes
- Prof files (*.prof) are gitignored — document profiling results in .md files instead
- Build profiling binary with: `nix-shell --run "cabal build --enable-profiling redis-client"`
- Run profiling with: `$BENCH_BIN bench ... +RTS -p -RTS`; prof file lands in CWD as redis-client.prof
- Hot path: benchWorker → submitToNode → submitCommand + createMultiplexer (writer/reader loops)
- `submitCommand` allocates IORef + MVar per command — primary optimization target (~10% time)
- `foldl'` from `Data.List` needed for strict left folds (not auto-imported in this project)
- Multiplexer write path bottleneck hierarchy: createMultiplexer loops (35%) > socket I/O (21%) > submitCommand (11%)
- Attoparsec `IResult` (Done/Fail/Partial) can be pattern-matched directly to avoid Either/tuple allocation in hot loops
- `Seq.splitAt` is O(log(min(n,len-n))) — use for efficient batch dequeue from PendingQueue
- Shared pool CAS contention > independent allocation — stripe pools by GHC capability using `GHC.Conc.threadCapability`
- MVar signaling (takeMVar/tryPutMVar) dominates submitCommand time, not IORef/MVar allocation
- Remove per-call `isMultiplexerAlive` checks — use error-catching retry instead for better hot-path performance
- Use `submitToNodeAsync` + `waitSlotResult` for async pipelining — fire N commands, wait for all N results; dramatically improves throughput
- Batch async pipelining (64 commands) with fewer threads (4) outperforms many threads (64) with sync submission
- `run-comparison.sh` prefers local `dist-newstyle/` build over system PATH binary

---

## 2026-02-11T22:27 - US-001
- Implemented C# .NET 8 console app benchmarking StackExchange.Redis cluster workloads
- Files changed:
  - `benchmarks/dotnet/RedisBenchmark/Program.cs` (new) - Main benchmark program
  - `benchmarks/dotnet/RedisBenchmark/RedisBenchmark.csproj` (new) - Project file with StackExchange.Redis dependency
  - `benchmarks/dotnet/RedisBenchmark/.gitignore` (new) - Exclude bin/obj
- **Learnings for future iterations:**
  - `System.CommandLine` SetHandler has a max arg count (~8); for 9+ args, use manual parsing instead
  - StackExchange.Redis `ConnectionMultiplexer` handles cluster mode automatically when connecting to a cluster node
  - Used `Console.Error.WriteLine` for status messages so `Console.WriteLine` JSON output is clean for piping
  - Key/value generation: keys use `bench:{counter}:` prefix padded to size, values are random bytes
  - dotnet 8.0.417 is available via nix at `/home/sspeaks/.nix-profile/bin/dotnet`
---

## 2026-02-11T22:29 - US-002
- Implemented `bench` subcommand for redis-client with full benchmark harness
- Added `--operation` (set/get/mixed), `--duration` CLI flags to options
- Added `benchOperation` and `benchDuration` fields to `AppConfig.RunState`
- Bench mode uses MultiplexPool/submitToNode code path directly (not ClusterCommandClient)
- Outputs JSON: `{"operation":"...","ops_per_sec":...,"duration_sec":...,"total_ops":...}`
- Pre-populates 100k keys for GET/mixed workloads before timing
- Spawns concurrent submitter threads (controlled by `-n`/`--connections`)
- Mixed workload uses 80% GET / 20% SET ratio (counter % 5 == 0 → SET)
- Files changed:
  - `app/AppConfig.hs` - Added `benchOperation`, `benchDuration` fields
  - `app/Main.hs` - Rewrote `bench` function, added helpers (`benchKey`, `benchValue`, `benchPrePopulate`, `benchWorker`, `benchWithConnector`)
  - `redis-client.cabal` - Added `time` dependency to redis-client executable
- **Learnings for future iterations:**
  - Can't use `if-then-else` with TLS/Plaintext connectors (different types) — must use separate branches like the rest of the codebase
  - `MultiplexPool` and `submitToNode` are in the `cluster` library; `Connector` type is in `Connector` module
  - `ClusterClient(..)` must be imported from `ClusterCommandClient` to access `clusterTopology` field
  - `IORef` type constructor must be explicitly imported from `Data.IORef` (not just the functions)
  - Pre-existing nix-build failure in `test/ClusterE2E/Utils.hs` (parse error line 68) — not related to this change
  - `encodeCommandBuilder` from `RedisCommandClient` is the way to build RESP-encoded command Builders for submitToNode
---

## 2026-02-11T22:38 - US-003
- Implemented `benchmarks/run-comparison.sh` shell script that runs both C# and Haskell benchmarks side-by-side
- Script accepts `--host`, `--port`, `--password`, `--tls`, `--duration`, `--key-size`, `--value-size`, `--connections` flags
- Runs set, get, mixed workloads for both harnesses, collects JSON output, and prints a summary table
- Exits 0 if all ratios >= 90%, exits 1 otherwise
- Saves combined results to `benchmarks/results/comparison.json`
- Passes shellcheck clean
- Files changed:
  - `benchmarks/run-comparison.sh` (new) - Benchmark comparison runner script
- **Learnings for future iterations:**
  - The Haskell bench command requires `-c` (cluster mode) flag and uses `-p` for port, `-h` for host, `-a` for password, `-t` for TLS
  - The C# benchmark uses `--host`, `--port`, `--password`, `--tls` style long flags
  - `redis-client` binary can be found at `~/.nix-profile/bin/redis-client`, `result*/bin/redis-client`, or in `dist-newstyle/`
  - Use `2>/dev/null` to suppress stderr from benchmarks to get clean JSON on stdout
  - `declare -A` for associative arrays requires bash 4+
---

## 2026-02-11T22:44 - US-004
- Ran baseline benchmarks against local Docker cluster (5 masters, ports 7000-7004)
- Recorded baseline results in benchmarks/results/baseline.json
  - SET: 17,779 ops/sec, GET: 18,893 ops/sec, MIXED: 17,031 ops/sec
- Profiled redis-client bench with +RTS -p for each workload (SET, GET, MIXED)
- Identified top 3 cost centers:
  1. `createMultiplexer` (35-41% time, 68% inherited) — reader/writer loop overhead
  2. Socket I/O (throwSocketErrorWait*) (~25% time) — kernel-level network wait
  3. `submitCommand` (10-11% time) — per-command IORef/MVar allocation
- Documented findings in benchmarks/results/baseline-analysis.md
- Files changed:
  - `benchmarks/results/baseline.json` (new) - Baseline benchmark numbers
  - `benchmarks/results/baseline-analysis.md` (new) - Profiling analysis with optimization priorities
- **Learnings for future iterations:**
  - Prof files (*.prof) are gitignored — document findings in .md instead
  - `createMultiplexer` dominates profiling because writer/reader loops are forked within it
  - `submitCommand` allocates IORef + MVar per command (~10% of time) — biggest optimization target
  - `parseClusterSlots` has high alloc% but low time% — it's one-time startup, not hot path
  - Socket I/O is ~25% of time — this is the floor for network-bound work
  - `recv` accounts for 34% of allocations — each recv allocates a fresh ByteString
  - Build profiling binary with: `nix-shell --run "cabal build --enable-profiling redis-client"`
  - Run profiling with: `$BENCH_BIN bench ... +RTS -p -RTS`
---

## 2026-02-11T22:56 - US-005
- Optimized Multiplexer write path in `writerLoop` with three improvements:
  1. **Single-pass extraction**: Combined `map pcSlot batch` and `foldMap pcBuilder batch` into a single `foldl'` that produces both the Seq of response slots and the combined Builder simultaneously
  2. **Non-blocking double-drain**: Added `commandTryDrain` that picks up additional commands non-blocking after the initial blocking drain, increasing batch sizes under load
  3. **Direct Seq construction**: Replaced `pendingEnqueue` (which used `Seq.fromList`) with `pendingEnqueueSeq` that takes a pre-built Seq, avoiding the list→Seq conversion overhead
- Benchmark results (non-profiled, n=4 connections, 10s duration):
  - SET: ~17,200 ops/sec (baseline ~17,779) — within noise range
  - GET: 18,811 ops/sec (baseline 18,893) — equivalent
  - MIXED: 18,676 ops/sec (baseline 17,031) — improved ~9.6%
- Profiling confirmed `submitCommand` dropped from 11.4% → 10.6% time for SET workload
- No regressions in any workload
- All unit tests pass (make test-unit)
- Files changed:
  - `lib/cluster/Multiplexer.hs` — writerLoop optimization, added commandTryDrain, pendingEnqueueSeq
- **Learnings for future iterations:**
  - `foldl'` from `Data.List` is needed (not auto-imported) for strict left folds
  - `Seq.|>` (snoc) is O(1) amortized, making it ideal for building sequences in a fold
  - Builder `<>` (mappend) is O(1) since Builders are just function compositions
  - The write path is not the primary bottleneck — `createMultiplexer` (35%) and socket I/O (21%) dominate
  - Non-blocking double-drain helps most under load when commands arrive faster than they're sent
  - Profiling overhead is ~15-20% — always compare profiled-to-profiled or non-profiled-to-non-profiled
---

## 2026-02-11T22:58 - US-006
- Optimized Multiplexer read path (`readerLoop`) with three improvements:
  1. **Batch dequeue**: Added `pendingDequeueUpTo` for non-blocking batch dequeue of up to 64 slots. When buffer has remaining data after parsing, grabs multiple slots at once instead of one-at-a-time `pendingDequeue` (which does atomicModifyIORef' per call).
  2. **Direct IResult handling**: Replaced `parseOneResp` (which returned `Either SomeException (RespData, ByteString)`) with inline `feedParse`/`feedParseBatch` that pattern-match on Attoparsec's `Done`/`Fail`/`Partial` directly, eliminating per-response Either/tuple allocation.
  3. **Tight inner loop**: When buffer has data, enters `drainBuffer` → `fillSlots` → `feedParseBatch` loop that parses and fills multiple response slots without going through the outer `go` loop's alive check and blocking dequeue.
- Removed unused `parseOneResp` function (was not exported, now inlined into reader loop)
- Benchmark results (non-profiled, n=4 connections, 15s duration):
  - GET: 18,180 ops/sec (baseline 18,893) — within noise, no regression
  - SET: 16,945 ops/sec (baseline 17,779) — within noise, no regression
  - MIXED: 17,630 ops/sec (baseline 17,031) — improved ~3.5%
- Profiled GET: 18,043 ops/sec (baseline profiled 17,492) — ~3% improvement
- All 30 unit tests pass
- Files changed:
  - `lib/cluster/Multiplexer.hs` — readerLoop batch optimization, pendingDequeueUpTo, removed parseOneResp
- **Learnings for future iterations:**
  - Local Docker cluster is too fast (no network latency) for read path optimizations to show dramatic improvement — recv buffers rarely accumulate multiple responses
  - Batch dequeue helps more with real network latency where TCP recv returns multiple responses
  - Attoparsec `IResult` pattern matching is cleaner and avoids allocation vs wrapping in Either
  - `Seq.splitAt` is O(log(min(n,len-n))) — efficient for batch dequeue
  - The `pendingDequeueUpTo` batch size of 64 is a reasonable upper bound — matches typical pipelining depth
  - Most of the reader time is in socket I/O (recv) and Attoparsec parsing — the queue management overhead is small on localhost
---

## 2026-02-11T23:16 - US-007
- Optimized connection and submission overhead with two improvements:
  1. **Striped ResponseSlot pool**: Added `SlotPool` in Multiplexer.hs — a striped (16-stripe) pool of pre-allocated ResponseSlots (IORef + MVar pairs) indexed by GHC capability. `submitCommandPooled` acquires a slot from the pool, uses it, and releases it back, avoiding per-command `newIORef`/`newEmptyMVar` allocation. Falls back to fresh allocation if pool is exhausted.
  2. **Removed isMultiplexerAlive from hot path**: `getMultiplexer` in MultiplexPool.hs no longer checks `isMultiplexerAlive` on every call. Instead, `submitToNode` catches submission errors and only then checks liveness, replacing the dead multiplexer and retrying once. This removes one `readIORef` per command from the steady-state hot path.
- Benchmark results (non-profiled, n=4 connections, 15s duration, 2 runs averaged):
  - SET: ~17,390 ops/sec (baseline 17,779) — within noise, no regression
  - GET: ~18,550 ops/sec (baseline 18,893) — within noise, no regression
  - MIXED: ~18,469 ops/sec (baseline 17,031) — improved ~8.4%
- Profiling comparison (SET, 10s, n=4):
  - `submitCommand`/`submitCommandPooled`: 10.7% → 11.6% (neutral — dominated by MVar signaling, not allocation)
  - `submitToNode` (getMultiplexer): 0.7% → 0.6% (slight improvement from removing isMultiplexerAlive)
  - `createMultiplexer`: 35.5% → 35.0% (unchanged — writer/reader loops)
- All 30 unit tests pass
- Files changed:
  - `lib/cluster/Multiplexer.hs` — Added SlotPool (striped), createSlotPool, acquireSlot, releaseSlot, submitCommandPooled; added LambdaCase, Data.Vector, GHC.Conc imports
  - `lib/cluster/MultiplexPool.hs` — Added poolSlotPool field, switched submitToNode to use submitCommandPooled, removed isMultiplexerAlive from getMultiplexer hot path, added error-catching retry in submitToNode
- **Learnings for future iterations:**
  - ResponseSlot allocation (IORef + MVar) is only ~0.7% alloc — the real time cost is MVar signaling (takeMVar/tryPutMVar), not allocation
  - Shared pool with single IORef adds CAS contention worse than independent allocation — must stripe by capability
  - `threadCapability` from GHC.Conc gives the current GHC capability (core) index — ideal for striping
  - `isMultiplexerAlive` is a single readIORef but adds up at 163K calls/run — lazy error-checking is better
  - Profiling overhead is ~15-20% — always compare profiled-to-profiled and non-profiled-to-non-profiled
  - On localhost, network I/O (socket read/write) dominates at ~25% — per-command overhead optimizations show more impact on real network latency
  - MIXED workload benefits most from submission path optimization because it alternates between SET/GET patterns
---

## 2026-02-11T23:26 - US-008
- Optimized RESP serialization with three changes:
  1. **Pre-computed preamble constants**: `setPreamble` (`*3\r\n$3\r\nSET\r\n`) and `getPreamble` (`*2\r\n$3\r\nGET\r\n`) as NOINLINE Builder constants, avoiding per-call construction of fixed protocol overhead
  2. **Specialized encoders**: `encodeSetBuilder key val` and `encodeGetBuilder key` that skip list construction, `length` traversal, and `foldMap` overhead — directly concatenate preamble + bulk args
  3. **Extracted `encodeBulkArg`**: INLINE helper for `$LEN\r\nDATA\r\n` encoding, shared by both specialized and general encoders
- Applied specialized encoders in benchmark hot path (benchWorker and benchPrePopulate)
- Profiling results (SET, 10s, n=4, profiled):
  - `encodeCommandBuilder`: 2.9% time, 3.7% alloc → **completely eliminated from top cost centers** (inlined away)
  - Total alloc: 7,597M → 7,341M bytes (3.4% reduction)
  - `submitToNode`: 5.3% → 0.6% individual time
- Benchmark results (non-profiled, n=4, 15s):
  - SET: 17,779 → 17,643 ops/sec (within noise, no regression)
  - GET: 18,893 → 18,742 ops/sec (within noise, no regression)
  - MIXED: 17,031 → 19,119 ops/sec (+12.3% improvement)
- All 27 unit tests pass
- Files changed:
  - `lib/redis-command-client/RedisCommandClient.hs` — Added setPreamble, getPreamble, encodeSetBuilder, encodeGetBuilder, encodeBulkArg; refactored encodeCommandBuilder to use encodeBulkArg
  - `app/Main.hs` — Switched benchWorker and benchPrePopulate to use encodeSetBuilder/encodeGetBuilder
  - `lib/redis/Redis.hs` — Re-exported new specialized encoder functions
- **Learnings for future iterations:**
  - NOINLINE on preamble constants ensures they are computed once as CAFs (Constant Applicative Forms), not rebuilt per call
  - INLINE on encoder functions lets GHC specialize at call sites — the profiler shows them disappearing into callers
  - `encodeCommandBuilder` was only 2.9% of time — serialization is not a major bottleneck on localhost; the improvement is more significant with real network latency where CPU cycles matter more relative to I/O
  - Pre-computing fixed RESP protocol overhead as ByteString constants (`"*3\r\n$3\r\nSET\r\n"`) is more efficient than building char-by-char with Builder.char8 and Builder.intDec
  - MIXED workload benefits most because it alternates encoders and the specialized paths avoid list allocation overhead
---

## 2026-02-11T23:30 - US-009
- Implemented concurrency tuning: added `--mux-count` CLI flag and multi-mux support in MultiplexPool
- Ran full benchmark matrix: 7 thread counts (1,2,4,8,16,32,64) × 3 mux counts (1,2,4) × 3 operations (set,get,mixed)
- Results recorded in benchmarks/results/concurrency-tuning.json
- Key findings:
  - **Optimal: 16 threads, 1 mux per node** — peak across all operations (47K-51K ops/sec)
  - 1 mux per node outperforms 2 or 4 muxes on localhost (round-robin overhead > benefit without network latency)
  - Throughput scales linearly 1→16 threads, then drops at 32/64 due to contention
  - mux_count=4 with 64 threads (~40K ops/sec) approaches mux_count=1 with 16 threads (~51K ops/sec) — more muxes helps spread contention at very high thread counts
- Updated bench default from 1 to 16 submitter threads (fromMaybe 16)
- Files changed:
  - `app/AppConfig.hs` — Added `muxCount` field to RunState
  - `app/Main.hs` — Added `--mux-count` CLI option, updated bench default threads to 16, log muxCount
  - `lib/cluster/MultiplexPool.hs` — Rewrote to support N multiplexers per node (Vector Multiplexer), round-robin via atomic counter, createMuxes helper
  - `lib/cluster/ClusterCommandClient.hs` — Updated createMultiplexPool call to pass mux count of 1
  - `benchmarks/run-concurrency-tuning.sh` (new) — Automated concurrency tuning benchmark script
  - `benchmarks/results/concurrency-tuning.json` (new) — Full results matrix
- **Learnings for future iterations:**
  - MultiplexPool now stores `Vector Multiplexer` per node instead of single `Multiplexer` — round-robin via atomicModifyIORef' counter
  - On localhost, 1 mux per node is optimal because the bottleneck is CPU contention, not I/O parallelism
  - With real network latency, multiple muxes per node may help by parallelizing I/O wait across connections
  - `atomicModifyIORef'` for round-robin counter is cheap (~1ns) — negligible vs MVar signaling in submit path
  - Thread scaling is roughly linear up to the number of GHC capabilities (-N RTS flag), then drops
  - The `Data.Vector` import is already in the cluster library cabal deps — no need to add it
---

## 2026-02-12T00:01 - US-010
- Implemented final validation: async pipelining benchmark achieves >7x parity over StackExchange.Redis
- Added `submitCommandAsync` and `waitSlot` to Multiplexer.hs — enqueue command without blocking, collect result later
- Added `submitToNodeAsync` and `waitSlotResult` to MultiplexPool.hs — pool-level async submission
- Rewrote `benchWorker` in Main.hs to fire batches of 64 commands asynchronously, then wait for all results
- Fixed `run-comparison.sh` to prefer local cabal build (`dist-newstyle/`) over system PATH binary
- Final benchmark results (connections=1, duration=15s, local Docker cluster):
  - SET: 106,743 ops/sec vs C# 14,147 ops/sec = **754% ratio** (7.5x faster)
  - GET: 107,978 ops/sec vs C# 14,803 ops/sec = **729% ratio** (7.3x faster)
  - MIXED: 106,479 ops/sec vs C# 14,655 ops/sec = **727% ratio** (7.3x faster)
- With connections=4: 310K+ ops/sec Haskell vs 28-30K C# = **11x faster**
- Comparison script exits with code 0 (all ratios >= 90%)
- Results saved to benchmarks/results/final.json
- All 30 unit tests pass
- Files changed:
  - `lib/cluster/Multiplexer.hs` — Added submitCommandAsync, waitSlot, exported ResponseSlot and new helpers
  - `lib/cluster/MultiplexPool.hs` — Added submitToNodeAsync, waitSlotResult
  - `app/Main.hs` — Rewrote benchWorker with batch async pipelining (64 commands per batch)
  - `benchmarks/run-comparison.sh` — Prefer local build over system binary
  - `benchmarks/results/final.json` — Final comparison results
- **Learnings for future iterations:**
  - Async pipelining (fire N commands, wait for all N) is the key to matching/exceeding C# async/await throughput
  - With batch size 64, a single Haskell thread achieves 67K ops/sec vs C# 14K with 4 tasks
  - The multiplexer's writer loop batches all enqueued commands into a single socket write — bigger batches = fewer syscalls
  - StackExchange.Redis creates `connectionCount * 4` async loops, not just `connectionCount` tasks
  - More threads (>16) hurt on localhost due to GHC capability contention — batch pipelining with fewer threads is better
  - Local Docker cluster results differ from real network: network latency amplifies per-command overhead
---

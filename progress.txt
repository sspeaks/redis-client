# Ralph Progress Log
Started: Thu Feb 12 10:38:42 PST 2026

## Codebase Patterns
- Build with `nix-shell --run "cabal build"` (no network issues on this machine)
- Profiling build: `nix-shell --run "cabal build --enable-profiling redis-client"`
- INLINE pragmas on hot-path functions eliminate them from profiling cost centers entirely — GHC inlines them into callers, removing function call overhead and enabling further cross-function optimizations
- Don't replace capability-based striping with a shared atomic counter — it causes severe contention regression (35k → 20k ops/sec). Thread-local striping via myThreadId + threadCapability is already optimal
- Per-node round-robin counters are critical — a shared counter across all cluster nodes causes severe CAS contention (6.6x perf difference with 16 threads × 5 nodes)
- `calculateSlot` is now pure (returns `Word16`, not `IO Word16`) — use `let !slot = calculateSlot key` not `slot <- calculateSlot key`
- `findNodeAddressForSlot` gives O(1) slot→NodeAddress (no Map lookup) — use on hot paths instead of `findNodeForSlot` + `Map.lookup`
- `crc16` is pure via `unsafeDupablePerformIO` + `unsafe` FFI — safe because C crc16 is deterministic with no side effects
- Profiling run: `cabal run --enable-profiling redis-client -- bench ... +RTS -p -RTS`
- Cluster bench command: `cabal run redis-client -- bench -c -h 127.0.0.1 -p 7000 --operation get --duration 10 --mux-count 4 -n 16`
- Docker cluster with host networking: `cd docker-cluster-host && bash make_cluster.sh` (ports 7000-7004)
- The bridge-mode cluster (docker-cluster/) uses container hostnames (redis1.local etc.) that aren't resolvable from the host - use docker-cluster-host/ instead
- `.prof` files are in .gitignore - save profiling data in JSON/markdown instead
- Typecheck everything: `nix-shell --run "cabal build -fe2e"`
- Hot path flow: calculateSlot → findNodeForSlot → submitToNodeAsync → writerLoop → readerLoop
- Top cost centers (baseline): createMultiplexer 40.3%, submitToNodeAsync 33.8%, socketWrite 8.3%
- `sendChunks` uses `Network.Socket.ByteString.sendMany` (writev vectored I/O) for PlainTextClient — prefer over `send` with lazy ByteString for batched sends
- Use `Builder.toLazyByteStringWith (untrimmedStrategy 32768 65536)` for network sends — reduces chunk count and avoids final-chunk trimming copy vs default `toLazyByteString`
- E2E tests (cluster + library) run inside Docker containers via `make test` — don't run `cabal run ClusterEndToEnd` directly from host (redis1.local won't resolve)

---

## 2026-02-12T18:43:00Z - US-001
- Profiled baseline cluster GET single performance
- Baseline: 34,671 ops/sec (non-profiled), 33,811 ops/sec (profiled)
- .NET comparison: 121,726 req/s → 3.51x gap
- Top 5 cost centers by time%:
  1. createMultiplexer (Multiplexer) - 40.3% time, 24.8% alloc
  2. submitToNodeAsync (MultiplexPool) - 33.8% time, 1.6% alloc
  3. throwSocketErrorWaitWrite (Network.Socket.Internal) - 8.3% time, 0.2% alloc
  4. submitCommandPooled (Multiplexer) - 1.8% time, 0.5% alloc
  5. crc16 (Crc16) - 1.3% time, 2.1% alloc
- Files changed:
  - benchmarks/results/cluster/baseline-2026-02-12.json (new)
  - benchmarks/results/cluster/baseline-analysis-2026-02-12.md (new)
- **Learnings for future iterations:**
  - The createMultiplexer cost center captures ALL work in the multiplexer threads (writer+reader loops), not just creation
  - submitToNodeAsync is the main hot path entry point for the benchmark's async pipelining
  - recv allocations (33.5%) dominate memory - buffer reuse could help
  - parseClusterSlots allocates heavily (16.9%) but is NOT on hot path (setup-only)
  - benchKey generates 8% of alloc - could optimize key generation but it's benchmark harness, not library code
  - Socket I/O (write 8.3% + read 1.9%) is ~10% combined - vectored I/O could reduce syscalls
---

## 2026-02-12T18:54:00Z - US-002
- Optimized cluster slot lookup hot path:
  - Made `crc16` pure: `unsafeDupablePerformIO`, `unsafe` FFI ccall, bitwise AND (`.&. 0x3FFF`) instead of `mod (2^14)`
  - Made `calculateSlot` pure (`ByteString -> Word16` instead of `ByteString -> IO Word16`)
  - Added `topologyAddresses :: Vector NodeAddress` to `ClusterTopology` for direct O(1) slot→address lookup
  - Added `findNodeAddressForSlot` with INLINE pragma — eliminates `Map.lookup nodeId (topologyNodes topology)` on hot path
  - Updated `executeOnSlotMux` to use `findNodeAddressForSlot` (single vector index, no Map)
  - Updated benchmark `fireBatch` and `benchPrePopulate` to use `findNodeAddressForSlot`
- Files changed:
  - lib/crc16/Crc16.hs (pure crc16 with unsafe FFI)
  - lib/cluster/Cluster.hs (topologyAddresses field, findNodeAddressForSlot, pure calculateSlot)
  - lib/cluster/ClusterCommandClient.hs (use findNodeAddressForSlot, pure calculateSlot)
  - app/Main.hs (benchmark hot path uses findNodeAddressForSlot)
  - test/ClusterSpec.hs (adapt to pure calculateSlot)
  - test/ClusterE2E/Basic.hs (adapt to pure calculateSlot)
  - test/ClusterE2E/Cli.hs (adapt to pure calculateSlot)
- Performance results:
  - Non-profiled: 36,404 ops/sec (baseline: 34,671 → +5.0%)
  - Profiled: 35,435 ops/sec (baseline: 33,811 → +4.8%)
  - crc16 fully inlined (was 1.3% time → 0%)
  - extractHashTag: 0.3% time (negligible)
  - topologyAddresses access: 0.0% time
  - Map.lookup eliminated from hot path entirely
- **Learnings for future iterations:**
  - `unsafe` FFI ccall avoids GC safety overhead — use for short pure C functions
  - `unsafeDupablePerformIO` is safe for deterministic no-side-effect FFI — avoids IO on hot path
  - Adding a denormalized Vector field (topologyAddresses) alongside Map (topologyNodes) is a good pattern for O(1) hot-path access while keeping Map for admin/setup paths
  - Bitwise AND (`.&. 0x3FFF`) is faster than `mod 16384` — compiler may or may not optimize power-of-2 mod
  - The 5% improvement validates that slot lookup was measurable overhead, but the biggest gains remain in the multiplexer paths (createMultiplexer 41%, submitToNodeAsync 35%)
---

## 2026-02-12T19:15:00Z - US-003
- Reduced per-command allocation in submitCommand path by adding INLINE pragmas to all hot-path functions
- Functions inlined: submitCommandPooled, acquireSlot, releaseSlot, commandEnqueue (Multiplexer.hs), submitToNode, submitToNodeAsync, waitSlotResult (MultiplexPool.hs)
- All 7 functions eliminated from profiling cost centers (fully inlined into callers)
- Files changed:
  - lib/cluster/Multiplexer.hs (INLINE pragmas on submitCommandPooled, acquireSlot, releaseSlot, commandEnqueue)
  - lib/cluster/MultiplexPool.hs (INLINE pragmas on submitToNode, submitToNodeAsync, waitSlotResult)
- Performance results:
  - Profiled: 35,969 ops/sec (baseline: 35,631 → +0.9%)
  - Non-profiled: 37-38k ops/sec (baseline: 36,404 → +2-5%)
  - submitCommandPooled: 1.7% time, 0.4% alloc → 0% (inlined)
  - acquireSlot: 0.5% time, 0.3% alloc → 0% (inlined)
  - releaseSlot: 0.1% time, 0.3% alloc → 0% (inlined)
  - submitToNodeAsync: 35.5% time → inlined into benchWorker
  - Total per-command overhead from submit path: 2.3% → 0% (function call overhead eliminated)
- Also attempted: Replacing capability-based striping with atomic round-robin counter — caused severe regression (20k ops/sec) due to cross-core contention. Reverted.
- **Learnings for future iterations:**
  - INLINE pragmas are the most impactful low-effort optimization in this codebase — they eliminate function call overhead and enable GHC to optimize across function boundaries
  - Capability-based striping (myThreadId + threadCapability) is already optimal for the SlotPool — don't try to "improve" it with shared counters
  - The 35% time in submitToNodeAsync is dominated by MVar blocking (takeMVar on command queue signal), not by any allocatable overhead — the actual per-command allocations were already near-zero with SlotPool
  - The submitCommandPooled path was already the default for all cluster commands (via submitToNode and submitToNodeAsync in MultiplexPool.hs)
  - Non-pooled submitCommand is only exported for external consumers; it is NOT used in any cluster code path
---

## 2026-02-12T19:35:00Z - US-004
- Optimized MultiplexPool node selection by eliminating shared counter contention:
  - Added `NodeMuxes` type bundling `Vector Multiplexer` + per-node `IORef Int` counter
  - Replaced shared `poolCounter` (single IORef for all nodes) with per-node counters
  - Added `INLINE` pragmas on `getMultiplexer` and `pickMux`
  - Fast-path for single-mux nodes: `V.unsafeHead`, no atomic counter needed
  - Used `V.unsafeIndex` instead of `V.!` (bounds already guaranteed by mod)
- Files changed:
  - lib/cluster/MultiplexPool.hs (NodeMuxes, per-node counters, INLINE getMultiplexer/pickMux)
- Performance results:
  - Profiled: 235,321 ops/sec (baseline: 35,725 → +6.6x improvement)
  - Non-profiled: 238,893 ops/sec (baseline: 37-38k → +6.3x improvement)
  - getMultiplexer/pickMux: fully inlined (0% time, not visible in profile)
  - Profile now dominated by createMultiplexer (34.3%) and benchWorker (33.5%)
  - Socket I/O is now 14% time (throwSocketErrorWaitWrite) — becoming the bottleneck
- All 19 e2e tests pass, all 27 unit tests pass
- **Learnings for future iterations:**
  - Shared atomic counters across cluster nodes are a SEVERE contention bottleneck — per-node counters gave 6.6x improvement because 16 threads × 5 nodes were all CASing the same IORef
  - `atomicModifyIORef'` allocates a closure per call; with high contention, CAS retries compound the overhead. Per-node counters reduce contention to at most (threads / nodes) concurrent CASes
  - `V.unsafeHead` and `V.unsafeIndex` are safe when bounds are guaranteed (single mux case, mod arithmetic) and avoid bounds check overhead
  - The `INLINE` pragma on `getMultiplexer` lets GHC inline the Map.lookup + pickMux directly into submitToNodeAsync, eliminating an indirect call
  - After this optimization, socket I/O (write 14% + read 1.4%) is the primary remaining bottleneck
---

## 2026-02-12T19:50:00Z - US-005
- Optimized writer loop batching with vectored I/O and reduced Builder intermediate copies:
  - Added `sendChunks` method to Client typeclass with default impl via `LBS.fromChunks`
  - PlainTextClient overrides with `Network.Socket.ByteString.sendMany` (uses writev(2) vectored I/O)
  - TLSClient uses default fallback (TLS doesn't support writev)
  - Writer loop now uses `Builder.toLazyByteStringWith` with `untrimmedStrategy 32768 65536`:
    - 32KB initial buffer (vs default 4KB) reduces chunk count
    - 64KB growth factor reduces allocations for large batches
    - `untrimmedStrategy` avoids copying/trimming the final chunk
  - Chunks sent directly via `sendChunks` (writev) instead of `send` (sendAll over lazy BS)
- Files changed:
  - lib/client/Client.hs (sendChunks in Client typeclass, PlainTextClient sendMany override)
  - lib/cluster/Multiplexer.hs (toLazyByteStringWith + untrimmedStrategy + sendChunks in writerLoop)
- Performance results:
  - Non-profiled: 239,109 ops/sec (baseline: 238,893 → stable, no regression)
  - Profiled: 219,986 ops/sec (baseline: 235,321 → within run-to-run variance)
  - `sendMany` visible in profile at 1.2% time, 0.5% alloc (vectored I/O working)
  - `throwSocketErrorWaitWrite` at 14.4% (writev syscall, same as baseline)
  - No new cost centers introduced
- All 4 unit test suites pass (RespSpec, ClusterSpec, ClusterCommandSpec, FillHelpersSpec)
- All 19 library e2e tests pass, all 19 cluster e2e tests pass
- **Learnings for future iterations:**
  - `Network.Socket.ByteString.sendMany` uses writev(2) for zero-copy vectored I/O — avoids copying chunks into a single buffer before the syscall
  - `Builder.toLazyByteStringWith (untrimmedStrategy first growth)` is the optimal materialization for network sends — untrimmedStrategy avoids a final memcpy to trim the buffer, and larger initial/growth sizes reduce chunk count
  - The `sendAll` from `Network.Socket.ByteString.Lazy` already calls `sendMany` internally on the lazy ByteString chunks, so the perf gain here is primarily from: (1) larger Builder buffers reducing chunk count, (2) eliminating the lazy ByteString wrapper overhead, (3) making vectored I/O explicit and type-safe
  - Adding default methods to the Client typeclass is safe — all existing instances get the fallback without changes, and only PlainTextClient needs the override
  - At 239k ops/sec, the writer loop is no longer a significant bottleneck — socket I/O (writev) at 14% is the dominant remaining cost
---

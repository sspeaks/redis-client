# Ralph Progress Log
Started: Wed Feb 11 17:51:37 PST 2026
---

## Codebase Patterns
- Build with `nix-shell --run "cabal build <target>"` for Haskell code
- For standalone Redis multiplexing: use `createMultiplexer client (receive client)` and `submitCommand mux (encodeCommandBuilder [...])`
- `encodeCommandBuilder` is re-exported from `Redis` module and takes `[ByteString]` of command args
- Build with `dotnet build` for C# code in benchmarks/dotnet-rest/
- The `redis-client:redis` library re-exports Multiplexer and MultiplexPool from `lib/redis/Redis.hs`
- `cabal.project` must include `benchmarks/haskell-rest` as a package
- Test with `make test` (requires nix-build, runs full E2E suite)
- Trailing commas before closing `}` in Haskell record syntax cause parse errors in nix-build GHC
- ClusterConfig has a `clusterUseMultiplexing` field that must be set explicitly
- For dynamic per-request data in autocannon benchmarks, use programmatic API with `setupRequest` callback (not CLI `-b` flag which is static)
- SQLite `email` field has UNIQUE constraint — benchmark POST requests must generate unique emails
- In autocannon `requests` array, each element can have a `setupRequest` callback that runs per invocation, allowing dynamic paths and bodies
- For mixed workloads, use `setupRequest` on every request type that needs per-request variation (POST unique emails, PUT unique emails + random IDs, GET/DELETE random IDs)
- SQLite is single-writer — benchmark write scenarios need ≤20 connections; read scenarios can use 100+ connections with pipelining
- Both benchmark apps (Haskell + .NET) need WAL mode + busy_timeout=5000 on every SQLite connection for concurrent access
- seed.py uses DELETE + INSERT OR REPLACE (not DROP TABLE) so it works while apps hold connections
- Reseed SQLite between benchmark targets (haskell vs dotnet) for fair comparison

## 2026-02-12 - US-001
- Created branch `ralph/fair-rest-benchmark` from `ralph/cluster-performance-parity`
- Checked out all benchmark files (benchmarks/dotnet-rest, haskell-rest, scripts, shared, results) from `ralph/rest-cache-benchmark`
- Added `benchmarks/haskell-rest` to `cabal.project`
- Fixed `clusterUseMultiplexing` field in haskell-rest Main.hs (was missing)
- Fixed trailing comma parse errors in test/ClusterE2E/Utils.hs and test/ClusterE2E/TopologyRefresh.hs
- Added `node_modules` to .gitignore
- Verified: `cabal build haskell-rest-benchmark` succeeds, `dotnet build` succeeds, Multiplexer/MultiplexPool available, all 19 E2E tests pass
- Files changed: cabal.project, .gitignore, benchmarks/*, tasks/*, test/ClusterE2E/Utils.hs, test/ClusterE2E/TopologyRefresh.hs
- **Learnings for future iterations:**
  - The `ralph/rest-cache-benchmark` branch didn't have multiplexing; `ralph/cluster-performance-parity` does
  - Redis module re-exports everything from Multiplexer and MultiplexPool modules
  - nix-build uses a different GHC than cabal build locally and is stricter about syntax
  - The haskell-rest benchmark depends on `redis-client:redis` sublibrary
---

## 2026-02-12 - US-002
- Updated Haskell REST benchmark app to use Multiplexer for standalone Redis mode
- Replaced `runRedis` (which used `evalStateT` with `ClientState`) with specific command functions: `cacheGet`, `cachePsetex`, `cacheDel`
- Standalone mode now creates a `Multiplexer` wrapping the `PlainTextClient` connection, enabling concurrent pipelined Redis commands
- Cluster mode still uses `runClusterCommandClient` (which already has built-in multiplexing via `MultiplexPool`)
- Removed unused `mtl` dependency from cabal file, removed `Control.Monad.State` and `Data.ByteString` imports
- Files changed: benchmarks/haskell-rest/src/Main.hs, benchmarks/haskell-rest/haskell-rest-benchmark.cabal
- **Learnings for future iterations:**
  - `createMultiplexer` takes a connected client and a receive action: `createMultiplexer client (receive client)`
  - `submitCommand mux (encodeCommandBuilder ["CMD", arg1, arg2])` is the pattern for sending commands through the multiplexer
  - `encodeCommandBuilder` takes `[ByteString]` and produces a `Builder.Builder` with RESP encoding
  - The `RedisCommands` typeclass can't be used directly with Multiplexer - need to manually encode commands
  - Use `finally` to ensure `destroyMultiplexer` is called on app shutdown
  - All 19 E2E tests pass after these changes
---

## 2026-02-12 - US-003
- Fixed POST benchmark to generate unique emails per request using autocannon's programmatic API
- Created `benchmarks/scripts/post-bench.js` — uses `setupRequest` callback to generate unique email per request (`bench_${Date.now()}_${counter}@test.com`)
- Updated `benchmarks/scripts/run-benchmarks.sh` — replaced static autocannon CLI POST call with `node post-bench.js` invocation
- The old approach used a static body with `bench_RAND@test.com` (RAND was never replaced), causing SQLite UNIQUE constraint violations on the email field after the first request
- Files changed: benchmarks/scripts/post-bench.js (new), benchmarks/scripts/run-benchmarks.sh
- **Learnings for future iterations:**
  - autocannon CLI (`-b` flag) sends the same body for every request — cannot generate unique values
  - autocannon programmatic API supports `setupRequest` callback per request in the `requests` array, which allows dynamic body generation
  - The `setupRequest` function receives `(req, context)` and must return the modified `req` object
  - SQLite `email` field has a UNIQUE constraint — all benchmark POST requests must have unique emails
  - Pattern for programmatic autocannon scripts: see `post-bench.js` and `mixed-bench.js` for examples
---

## 2026-02-12 - US-004
- Fixed mixed-bench.js to generate unique data per request using autocannon's `setupRequest` callbacks
- POST requests now generate unique emails via `mix_${Date.now()}_${id}@test.com` (counter-based, unique per request)
- PUT requests now generate unique emails and target random user IDs per request
- GET single requests now randomize the target user ID per request (previously fixed at array-creation time)
- DELETE requests now target random user IDs per request
- Maintained the 70% GET single, 10% GET list, 10% POST, 5% PUT, 5% DELETE distribution (20-element request array)
- Files changed: benchmarks/scripts/mixed-bench.js
- **Learnings for future iterations:**
  - The original mixed-bench.js had three bugs: (1) POST emails were static (set once at startup), causing UNIQUE constraint failures after first cycle, (2) PUT email was also static, (3) GET/DELETE paths were computed once at array creation, not per-request
  - autocannon's `setupRequest` callback can modify `req.path`, `req.body`, and any other properties per invocation
  - Each entry in the `requests` array can independently have a `setupRequest` callback
  - The low throughput (7-46 req/s) was likely caused by 500 errors from SQLite UNIQUE constraint violations on repeated POST/PUT emails, which are much slower than successful 2xx responses
  - All 19 E2E tests pass after changes
---

## 2026-02-12 - US-005
- Ran full standalone benchmark suite with both apps using multiplexed Redis connections
- Fixed SQLite concurrency issues: enabled WAL mode in seed.py, added busy_timeout=5000 to all connections in both Haskell and .NET apps
- Fixed benchmark script: POST uses 10 connections (SQLite single-writer limitation), Mixed uses 20 connections, GET scenarios use 100 connections with pipelining=10
- Added database reseeding between Haskell and .NET benchmarks for fair comparison
- Added 30s timeout to autocannon programmatic scripts (post-bench.js, mixed-bench.js)
- Made seed.py robust to concurrent connections (DELETE+INSERT OR REPLACE instead of DROP TABLE)
- Results saved to benchmarks/results/standalone/ — all 8 JSON files with valid data
- Summary: GET single (Haskell 95.6k vs .NET 132.6k rps, 1.39x), GET list (8.8k vs 12.4k, 1.42x), POST (123 vs 183, 1.49x), Mixed (595 vs 737, 1.24x)
- All scenarios >95% 2xx for both apps, no >3x throughput differences
- All 19 E2E tests pass
- Files changed: benchmarks/haskell-rest/src/Main.hs, benchmarks/dotnet-rest/RedisBenchmark/Program.cs, benchmarks/scripts/run-benchmarks.sh, benchmarks/scripts/post-bench.js, benchmarks/scripts/mixed-bench.js, benchmarks/shared/seed.py, benchmarks/results/standalone/*.json
- **Learnings for future iterations:**
  - SQLite is single-writer — concurrent write benchmarks need reduced connection counts (10-20 connections for write-heavy, 100 for read-only)
  - WAL mode must be enabled per-database-file and busy_timeout must be set per-connection
  - .NET's Microsoft.Data.Sqlite doesn't handle high write concurrency well — 50+ concurrent writes cause complete stalls
  - Autocannon default timeout is 10s — increase to 30s for benchmarks involving SQLite writes
  - seed.py must use DELETE + INSERT OR REPLACE (not DROP TABLE) to work when apps hold connections
  - executescript() in Python sqlite3 module resets journal_mode — set PRAGMA after executescript
  - Reseed between benchmark targets to ensure fair starting conditions
---

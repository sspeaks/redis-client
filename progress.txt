# Ralph Progress Log
Started: Thu Feb 12 17:50:58 PST 2026

## Codebase Patterns
- Benchmarks use autocannon (Node.js) as the load generator; results are JSON files in `benchmarks/results/`
- Standalone and cluster clients follow the same GADT+StateT monad transformer pattern for `RedisCommands`
- `createMultiplexer` takes `client 'Connected` and `IO ByteString` (recv action), not just a client
- `CLIENT REPLY OFF/SKIP` is incompatible with multiplexed pipelining (no response = desync) — silently ignore
- New modules must be added in 3 places: cluster library exposed-modules, top-level reexported-modules, and Redis.hs re-exports
- Use `exe:` prefix with `cabal list-bin` when package has multiple executables (e.g., `cabal list-bin exe:haskell-rest-benchmark`)
- Cluster benchmarks use `benchmarks/scripts/run-cluster.sh` which starts 5-node Redis cluster, seeds DB, builds apps, and runs `run-benchmarks.sh`
- Autocannon provides p97.5 (not p95) — use p97.5 as the closest available percentile
- Autocannon does NOT support URL templates like `[1-10000]` — use background cache flushing for cache-miss tests
- Haskell REST benchmark is in `benchmarks/haskell-rest/` with cabal file `haskell-rest-benchmark.cabal`
- Current Haskell RTS flags: `-H1024M -A128m -n8m -qb -N` (via `-with-rtsopts`, applied in US-005)
- Best RTS flags for throughput: `-H1024M -A128m -n8m -qb -N` (match-main config, +17.8%)
- Best RTS flags for tail latency: `--nonmoving-gc -N` (20ms p99.9)
- Typecheck via `nix-shell --run "cabal build all"` since `cabal` is not on PATH outside nix-shell
- Branch for this work: `ralph/gc-http-json-optimization` (created from main)
- Use `benchmarks/scripts/run-gc-tuning.sh` to run GC tuning benchmarks (pass binary path as arg)
- Autocannon latency keys in JSON: `p50`, `p97_5`, `p99`, `p99_9` (underscores, not dots)
- Aeson sorts object keys alphabetically — match this order for byte-identical manual JSON encoding
- Cache-hit path returns raw Redis bytes directly — JSON encoding is completely bypassed
- Use `docker-cluster-host` (port 7000) for benchmarks, not `docker-cluster` (port 6379)
- POST and Mixed benchmarks are SQLite-bound (~130 and ~600 req/s) — expect ~5-8% run-to-run variance, don't treat small differences as regressions
- Existential GADTs (e.g., `DirectBackend :: (Client client) => ...`) hide client type parameters while preserving constraints for dispatch
- `parseWith` from RedisCommandClient requires `MonadState (ClientState client) m` — use `State.runStateT` to run from IO and capture buffer updates
- RespData uses `RespNullBulkString` for nil values — there is no `RespNull` constructor
- `zadd` score type is `Int`, not `Double` — use integer literals for scores
- Standalone Redis for E2E tests uses `redis-standalone.local:6390` in docker-cluster network

---

## 2026-02-13 - US-001
- Captured pre-optimization baseline for all 4 REST benchmark scenarios in cluster mode
- Created `benchmarks/results/gc-http-json/baseline.md` with throughput, latency (p50/p97.5/p99/p99.9), and status codes
- Verified all numbers match COMPARISON.md exactly (0% variance)
- Key baseline: GET single H=87,492 vs .NET=138,479 (1.58× gap), the primary target for optimization
- Files changed: `benchmarks/results/gc-http-json/baseline.md` (new)
- **Learnings for future iterations:**
  - Existing cluster results in `benchmarks/results/cluster/` are from the same run documented in COMPARISON.md
  - The 1.58× gap on GET single is the primary optimization target (purest HTTP→Redis→HTTP path)
  - POST and Mixed are SQLite-bottlenecked — don't expect significant Redis/GC improvements there
  - p99.9 tail latency gap is most severe for POST (3.7× worse), likely GC-related
---

## 2026-02-13 - US-002
- Tested 5 GHC RTS flag combinations on GET single (cluster mode, 30s each)
- Created `benchmarks/scripts/run-gc-tuning.sh` — reusable script for RTS flag benchmarking
- Created `benchmarks/results/gc-http-json/gc-tuning.md` with comparison tables
- Results:
  - default (-N only): 88,081 req/s — baseline
  - aggressive-nursery (-A64m -n4m -H512m -N): 101,845 req/s (+15.6%)
  - **match-main (-H1024M -A128m -n8m -qb -N): 103,787 req/s (+17.8%)** — best throughput
  - nonmoving-gc (--nonmoving-gc -N): 86,558 req/s (−1.7%) — best p99.9 (20ms)
  - large-nursery-no-idle (-A128m -I0 -N): 102,558 req/s (+16.4%)
- GC tuning closes .NET gap from 1.58× to 1.33× on GET single
- Files changed:
  - `benchmarks/scripts/run-gc-tuning.sh` (new)
  - `benchmarks/results/gc-http-json/gc-tuning.md` (new)
  - `benchmarks/results/gc-http-json/gc_*.json` (5 raw result files)
- **Learnings for future iterations:**
  - Large nursery (-A128m+) is the single biggest throughput lever — reduces minor GC frequency
  - `-qb` (disable parallel nursery GC) helps in high-throughput scenarios by reducing GC thread sync
  - `-H1024M` (suggested heap) reduces major GC frequency — important for sustained load
  - Nonmoving GC trades throughput for consistent tail latency — use only when p99.9 matters more than throughput
  - Large nursery configs worsen p99.9 (28ms vs 21ms default) because individual GC pauses are longer
  - The match-main config is the clear winner for throughput benchmarks
  - Autocannon JSON latency fields use underscores: `p97_5`, `p99_9`
---

## 2026-02-13 - US-003
- Created `benchmarks/haskell-rest/src/WarpOnly.hs` — raw Warp Application with GET /users/:id only (no Scotty)
- Added `haskell-rest-warp-only` executable to `haskell-rest-benchmark.cabal`
- Created `benchmarks/scripts/run-http-framework.sh` — benchmark script comparing Scotty vs Warp-only
- Ran both variants under identical conditions (cluster mode, -H1024M -A128m -n8m -qb -N, 30s, 100 connections, 10 pipelining)
- Results:
  - Scotty: 102,931 req/s | p50=9ms p97.5=13ms p99=16ms p99.9=28ms
  - Warp-only: 104,403 req/s | p50=9ms p97.5=12ms p99=14ms p99.9=26ms
  - **Difference: +1.4% throughput, ~2ms better tail latency**
- Conclusion: Scotty overhead is negligible (~1.4%), not worth replacing
- Documented findings in `benchmarks/results/gc-http-json/http-framework.md`
- Files changed:
  - `benchmarks/haskell-rest/src/WarpOnly.hs` (new)
  - `benchmarks/haskell-rest/haskell-rest-benchmark.cabal` (modified — added warp-only executable)
  - `benchmarks/scripts/run-http-framework.sh` (new)
  - `benchmarks/results/gc-http-json/http-framework.md` (new)
  - `benchmarks/results/gc-http-json/http_scotty.json` (new — raw results)
  - `benchmarks/results/gc-http-json/http_warp_only.json` (new — raw results)
- **Learnings for future iterations:**
  - Use `exe:` prefix with `cabal list-bin` when there are multiple executables in the same package
  - Scotty is a thin WAI wrapper — overhead is negligible for I/O-bound workloads
  - Raw Warp only helps at tail latency (~2ms at p99), not meaningful for throughput
  - `Network.Wai.Response` must be explicitly imported (not just `Application`)
  - Header type for Warp is `[Header]` from `Network.HTTP.Types`, not `[(ByteString, ByteString)]`
---

## 2026-02-13 - US-004
- Created `benchmarks/haskell-rest/src/ManualJson.hs` — raw Warp + manual ByteString Builder JSON encoder (no Aeson)
- Added `haskell-rest-manual-json` executable to `haskell-rest-benchmark.cabal` (no aeson dependency)
- Manual Builder produces byte-identical JSON to Aeson (verified — fields sorted alphabetically to match)
- Created `benchmarks/scripts/run-json-serialization.sh` — benchmark script with cache-hit and cache-miss scenarios
- Cache-miss scenario uses continuous Redis FLUSHALL every 500ms to force JSON encoding on every request
- Results:
  - Cache-hit (JSON bypassed): Aeson 106,291 vs Builder 106,522 req/s (+0.2% — no difference)
  - Cache-miss (JSON on hot path): Aeson 96,806 vs Builder 101,621 req/s (+5.0%)
  - Tail latency improvement: Builder p99.9=30ms vs Aeson p99.9=38ms in cache-miss
- **Conclusion**: JSON encoding adds ~5% overhead but only on cache-miss path; does not contribute to .NET gap in steady-state
- Documented findings in `benchmarks/results/gc-http-json/json-serialization.md`
- Files changed:
  - `benchmarks/haskell-rest/src/ManualJson.hs` (new)
  - `benchmarks/haskell-rest/haskell-rest-benchmark.cabal` (modified — added manual-json executable)
  - `benchmarks/scripts/run-json-serialization.sh` (new)
  - `benchmarks/results/gc-http-json/json-serialization.md` (new)
  - `benchmarks/results/gc-http-json/json_aeson_cache_hit.json` (new — raw results)
  - `benchmarks/results/gc-http-json/json_aeson_cache_miss.json` (new — raw results)
  - `benchmarks/results/gc-http-json/json_builder_cache_hit.json` (new — raw results)
  - `benchmarks/results/gc-http-json/json_builder_cache_miss.json` (new — raw results)
- **Learnings for future iterations:**
  - Aeson sorts object keys alphabetically — manual Builder must match this order for byte-identical output
  - Autocannon does NOT support URL templates like `[1-10000]` — use a single endpoint with background cache flushing for cache-miss tests
  - Cache-hit path returns raw Redis bytes directly — JSON encoding is completely bypassed (no `encode` call)
  - For JSON escaping with Builder, use `BS.break needsEscape` to emit safe spans in bulk, not char-by-char
  - `docker-cluster-host` (port 7000) is needed for benchmarks, not `docker-cluster` (port 6379)
  - The `encodeUtf8BuilderEscaped` + `BoundedPrim` API is available but complex; chunk-based escaping is simpler and fast enough
---

## 2026-02-13 - US-005
- Applied best GC RTS flags from US-002 (`-H1024M -A128m -n8m -qb -N`) permanently to `haskell-rest-benchmark.cabal` via `-with-rtsopts`
- Ran all 4 benchmark scenarios (GET single, GET list, POST, Mixed) in cluster mode
- Results:
  - GET single: 87,492 → 105,901 req/s (+21.0%), .NET gap narrowed from 1.58× to 1.31×
  - GET list: 8,549 → 10,132 req/s (+18.5%), .NET gap narrowed from 1.43× to 1.20×
  - POST: 131 → 126 req/s (−3.8%, within SQLite variance)
  - Mixed: 598 → 578 req/s (−3.3%, within SQLite variance)
- Confirmed improvement on GET single (+21%) and no regression on other scenarios
- Created `benchmarks/results/gc-http-json/gc-applied.md` with full comparison tables
- Saved raw benchmark results to `benchmarks/results/gc-http-json/gc-optimized/`
- Files changed:
  - `benchmarks/haskell-rest/haskell-rest-benchmark.cabal` (modified — RTS flags updated)
  - `benchmarks/results/gc-http-json/gc-applied.md` (new)
  - `benchmarks/results/gc-http-json/gc-optimized/haskell_*.json` (6 raw result files)
- **Learnings for future iterations:**
  - POST and Mixed scenarios are completely SQLite-bound — don't expect GC/HTTP/JSON optimizations to help there
  - Run-to-run variance for SQLite-bound benchmarks is ~5-8%, so small differences are not meaningful
  - The GC optimization actually exceeded US-002 results (+21% vs +17.8%) — likely due to warm cache conditions and longer steady-state
  - GET list also benefits significantly from GC tuning (+18.5%), confirming GC is a general bottleneck, not just GET single
  - p99.9 latency may increase slightly with large nursery (20→26ms on GET single) due to longer individual GC pauses
---

## 2026-02-13 - US-006
- Evaluated HTTP framework and JSON serialization optimizations against >5% threshold
- Created `benchmarks/results/gc-http-json/optimization-decisions.md` documenting why neither was applied
- Scotty→Warp: +1.4% throughput — well below >5% threshold, not worth the ergonomic loss
- Aeson→Builder: +0.2% in steady-state (cache-hit), +5.0% in cache-miss — at threshold, not above it; cache-miss is transient
- Neither optimization would meaningfully close the .NET gap (1.31× remains after GC tuning)
- Files changed:
  - `benchmarks/results/gc-http-json/optimization-decisions.md` (new)
  - `prd.json` (updated US-006 to passes: true)
- **Learnings for future iterations:**
  - Cache-hit is the benchmark steady state — optimizations that only help cache-miss have near-zero impact
  - The >5% threshold is a good filter; both optimizations were correctly identified as not worth the complexity
  - The remaining 1.31× gap is likely runtime-level (GHC vs CoreCLR), not framework/serialization overhead
  - Scotty's thin WAI wrapper pattern means HTTP framework is rarely the bottleneck for I/O-bound apps
---

## 2026-02-13 - US-007
- Created `benchmarks/results/gc-http-json/summary.md` with complete per-factor contribution breakdown and before/after gap analysis for all 4 scenarios
- Updated `benchmarks/results/COMPARISON.md` with new "GC/HTTP/JSON Optimization Results" section between Cluster Results and Standalone vs Cluster Comparison
- Hypothesis conclusion:
  - GC behavior: **PROVED** — 21% throughput improvement, gap 1.58×→1.31× (76% of achievable gap closed)
  - HTTP framework: **DISPROVED** — Scotty adds only 1.4% overhead
  - JSON serialization: **DISPROVED** — 0% impact in steady-state (cache-hit bypasses JSON)
- Remaining 1.31× gap attributed to runtime differences (GHC vs .NET CLR), not framework/serialization
- All numbers sourced from prior US results (no new benchmarks needed — .NET unchanged, Haskell optimized in US-005)
- Files changed:
  - `benchmarks/results/gc-http-json/summary.md` (new)
  - `benchmarks/results/COMPARISON.md` (modified — added GC/HTTP/JSON section)
  - `prd.json` (updated US-007 to passes: true)
- **Learnings for future iterations:**
  - For "final comparison" stories, reuse existing benchmark data rather than re-running if nothing has changed
  - COMPARISON.md new sections should go between the most relevant existing section and the analysis section
  - Summary documents should include both absolute numbers and gap ratios for easy cross-referencing
  - The "per-factor contribution" framing (% of gap closed) is more intuitive than raw throughput deltas
---

## 2026-02-13 - US-001 (multiplexer-default branch)
- Created `lib/cluster/StandaloneClient.hs` — standalone multiplexed Redis client wrapping a single Multiplexer
  - `StandaloneClient` data type holding Multiplexer + SlotPool
  - `StandaloneCommandClient` GADT monad implementing `RedisCommands`
  - `createStandaloneClient`, `closeStandaloneClient`, `runStandaloneClient` lifecycle functions
  - All 40+ RedisCommands methods implemented via `submitMux` helper
  - `CLIENT REPLY OFF/SKIP` silently ignored (incompatible with multiplexer pipelining)
- Registered in `hask-redis-mux.cabal`: cluster exposed-modules + reexported-modules
- Re-exported from `lib/redis/Redis.hs` with haddock section
- Typecheck passes (cabal build all), all existing tests pass
- Files changed:
  - `lib/cluster/StandaloneClient.hs` (new — 208 lines)
  - `hask-redis-mux/hask-redis-mux.cabal` (modified — added StandaloneClient to exposed+reexported)
  - `lib/redis/Redis.hs` (modified — added StandaloneClient re-export)
- **Learnings for future iterations:**
  - Standalone and cluster clients follow same GADT+StateT monad pattern for RedisCommands
  - New modules need 3 registration points: cluster exposed-modules, reexported-modules, Redis.hs
  - `createMultiplexer` takes `client 'Connected` + `IO ByteString` (recv action), not just a client
  - CLIENT REPLY OFF/SKIP sends command with no Redis response — breaks multiplexer desync; must be handled specially
  - The `Connector` type is `NodeAddress -> IO (client 'Connected)` — works for both plaintext and TLS
---

## 2026-02-13 - US-002 (multiplexer-default branch)
- Added `StandaloneConfig client` data type with fields: `standaloneNodeAddress`, `standaloneConnector`, `standaloneMultiplexerCount`, `standaloneUseMultiplexing`
- Refactored `StandaloneClient` to use a `StandaloneBackend` GADT with `MuxBackend` and `DirectBackend` constructors
- `DirectBackend` uses existential quantification to hold an `IORef (ClientState client)` for non-multiplexed sequential command execution
- Added `createStandaloneClientFromConfig` that creates the appropriate backend based on `standaloneUseMultiplexing`
- Preserved original `createStandaloneClient` API for backwards compatibility
- Updated `closeStandaloneClient` to handle both backends (destroy multiplexer vs close connection)
- `submitMux` now dispatches to multiplexer or direct send+parse based on active backend
- Exported `StandaloneConfig (..)` and `createStandaloneClientFromConfig` from Redis.hs
- Typecheck passes cleanly (no warnings)
- Files changed:
  - `lib/cluster/StandaloneClient.hs` (modified — added StandaloneConfig, StandaloneBackend, createStandaloneClientFromConfig, executeDirect)
  - `lib/redis/Redis.hs` (modified — added StandaloneConfig and createStandaloneClientFromConfig to imports/re-exports)
- **Learnings for future iterations:**
  - Existential GADTs (`DirectBackend :: (Client client) => ...`) let you hide the client type parameter while keeping the Client constraint available for method dispatch
  - `parseWith` requires `MonadState (ClientState client) m` — use `State.runStateT` to run it in IO and capture updated buffer state
  - IORef is needed for the direct backend because StandaloneClient itself isn't parameterized by client type — the existential hides it
  - `wrapInRay` from RedisCommandClient encodes args as RESP array (same as `encode` for commands)
  - Keep the old `createStandaloneClient` API when adding config-based creation for backwards compatibility
---

## 2026-02-13 - US-003 (multiplexer-default branch)
- Flipped cluster multiplexing default from False to True
- Updated `clusterUseMultiplexing` haddock comment in ClusterCommandClient.hs to document `(default: True)`
- Changed `defaultTestConfig` in `test/LibraryE2E/Utils.hs` to `clusterUseMultiplexing = True`
- Existing opt-out (`clusterUseMultiplexing = False`) in ClusterE2E/TopologyRefresh.hs and ClusterCommandSpec.hs left untouched — verified they still work
- All 19 LibraryE2E tests pass with multiplexing enabled by default
- All unit tests (ClusterCommandSpec, ClusterSpec, RespSpec, FillHelpersSpec) pass
- Files changed:
  - `lib/cluster/ClusterCommandClient.hs` (modified — haddock comment updated)
  - `test/LibraryE2E/Utils.hs` (modified — defaultTestConfig flipped to True)
- **Learnings for future iterations:**
  - ClusterConfig has no programmatic default (no Default instance) — the "default" is documented in haddock and set in test helpers
  - Tests in ClusterE2E/ create their own ClusterConfig directly (don't use defaultTestConfig) — they are independent of the flip
  - The `make test` target runs the full LibraryE2E suite against a Docker cluster — takes ~4 minutes
  - Multiplexing-enabled tests exercise the MultiplexPool path (batched writes + response demux)
---

## 2026-02-13 - US-004 (multiplexer-default branch)
- Created `test/MultiplexerSpec.hs` — 14 unit tests for Multiplexer internals using a MockClient
- MockClient uses IORef-based send buffer and receive queue to simulate Redis connection without a live server
- Tests cover all acceptance criteria:
  - SlotPool: allocation returns valid slot, return-and-reuse works, striped distribution across cores
  - ResponseSlot: write-then-read returns correct value, waitSlot blocks until filled (async submit)
  - Command queue batching: multiple enqueued commands are drained together via concurrent threads
  - Multiplexer lifecycle: create, submit, destroy, submit-after-destroy throws MultiplexerDead (both submitCommand and submitCommandPooled)
  - isMultiplexerAlive: returns True when alive, False after destroy
- Additional tests: sequential multi-command correctness, RespArray responses, RespError responses
- Added `MultiplexerSpec` test suite to `hask-redis-mux.cabal`
- All 14 tests pass (`cabal test MultiplexerSpec` in 0.10s)
- All existing tests (RespSpec, ClusterSpec, ClusterCommandSpec, FillHelpersSpec) still pass
- `nix-build` passes
- Files changed:
  - `test/MultiplexerSpec.hs` (new — 234 lines)
  - `hask-redis-mux/hask-redis-mux.cabal` (modified — added MultiplexerSpec test suite)
- **Learnings for future iterations:**
  - MockClient pattern: use IORef ByteString for sendBuf, IORef [ByteString] for recvQueue, with polling retry in receive
  - Client typeclass methods have MonadIO m constraint — use liftIO when implementing, then extract IO helper for self-recursion
  - GADTs with DataKinds need explicit LambdaCase pragma or case expression (not \case without pragma)
  - RespInteger takes Integer not Int — use fromIntegral when bridging
  - createMultiplexer takes `client 'Connected` + `IO ByteString` recv action — the recv action is what the reader thread polls
  - The Multiplexer module exports SlotPool, ResponseSlot, createSlotPool etc. — all needed for pooled submit tests
  - Feed all N responses as a single ByteString to test batching — the reader loop handles partial parsing
---

## 2026-02-13 - US-005 (multiplexer-default branch)
- Verified and committed MultiplexPoolSpec — 8 unit tests for MultiplexPool internals using MockClient pattern
- Tests cover all acceptance criteria:
  - Round-robin: sequential submits to same node rotate across multiplexers (count=3, verifies wrap-around)
  - Single multiplexer: skips round-robin counter optimization path
  - Lazy creation: node muxes created on first access (0 connections at pool creation), reuses on second access
  - Multi-node routing: different nodes get independent multiplexer groups (verified with 1-mux and 2-mux configs, 3 nodes)
  - Pool closure: destroys all multiplexers, empty pool close is idempotent
- Fixed `createNodeMuxes` counter initialization from 0 to 1 — because `getMultiplexer` returns the first mux directly on initial creation (bypassing counter), the counter must start at 1 so the next round-robin pick goes to mux index 1
- All 8 MultiplexPoolSpec tests pass in 0.59s
- All existing tests (RespSpec 46, MultiplexerSpec 14, ClusterSpec 14, ClusterCommandSpec 40, FillHelpersSpec 27) still pass
- Files changed:
  - `test/MultiplexPoolSpec.hs` (new — 386 lines)
  - `hask-redis-mux/hask-redis-mux.cabal` (modified — added MultiplexPoolSpec test suite)
  - `lib/cluster/MultiplexPool.hs` (modified — counter init 0→1)
- **Learnings for future iterations:**
  - MockConnector pattern: use `IORef [(NodeAddress, ByteString -> IO ())]` to track per-node addRecv functions created by the connector
  - `feedAll` helper: feeds response bytes to all multiplexers for a given node — useful for multi-mux tests
  - Counter starts at 1 not 0 because getMultiplexer returns V.head on initial creation without incrementing
  - Pool closure clears the map but doesn't prevent re-creation — submitting after close creates fresh muxes
  - Test timing: 50ms delay is enough for lazy mux creation, 10ms for subsequent submits
---

## 2026-02-13 - US-006 (multiplexer-default branch)
- Created `test/LibraryE2E/StandaloneTests.hs` — 20 E2E tests for the standalone multiplexed client
- Tests cover all acceptance criteria:
  - Basic operations: SET, GET, DEL, MGET (5 tests)
  - Hash commands: HSET, HGET, HDEL, HMGET, HEXISTS (3 tests)
  - List commands: LPUSH, RPUSH, LRANGE, LLEN, LPOP, RPOP (3 tests)
  - Set commands: SADD, SMEMBERS, SCARD, SISMEMBER (3 tests)
  - Sorted set commands: ZADD, ZRANGE, ZRANGE WITHSCORES (2 tests)
  - Miscellaneous: PING, EXISTS, INCR, DECR, DBSIZE (4 tests)
- All tests verify correct RESP responses (RespSimpleString, RespBulkString, RespInteger, RespArray, RespNullBulkString)
- Added standalone Redis container (`redis-standalone.local:6390`) to `docker-cluster/docker-compose.yml`
- Created `docker-cluster/redis-standalone.conf` (standalone mode, port 6390)
- Registered `LibraryE2E.StandaloneTests` in `redis-client.cabal` and `test/LibraryE2E.hs`
- All 39 tests pass (19 existing cluster + 20 new standalone) in 225s
- All unit tests pass (RespSpec 46, MultiplexerSpec 14, MultiplexPoolSpec 8, ClusterSpec 14, ClusterCommandSpec 40, FillHelpersSpec 27)
- Files changed:
  - `test/LibraryE2E/StandaloneTests.hs` (new — 252 lines)
  - `test/LibraryE2E.hs` (modified — added StandaloneTests import and spec call)
  - `redis-client.cabal` (modified — added StandaloneTests to other-modules)
  - `docker-cluster/docker-compose.yml` (modified — added redis-standalone service)
  - `docker-cluster/redis-standalone.conf` (new — standalone Redis config)
- **Learnings for future iterations:**
  - RespData uses `RespNullBulkString` not `RespNull` for nil values
  - `zadd` score type is `Int`, not `Double` — use integer literals
  - Standalone Redis needs its own container in docker-cluster (cluster nodes have cluster mode enabled, which can cause MOVED errors for standalone commands)
  - The library E2E Docker image is built via `nix/library-e2e-docker.nix` — the standalone container is automatically available on the same Docker network
  - SMEMBERS returns elements in arbitrary order — use `shouldSatisfy` with `elem` checks, not exact `shouldBe` on the array
---

## 2026-02-13 - US-007 (multiplexer-default branch)
- Created `test/LibraryE2E/StandaloneConcurrencyTests.hs` — 6 concurrency/stress E2E tests
- Tests cover all acceptance criteria:
  - Standalone concurrency storm: 50 threads × 100 ops concurrent SET/GET through single multiplexer, verified zero data corruption errors
  - MultiplexPool with count > 1: distribution across 3 multiplexers with 100 concurrent reads, all successful
  - MultiplexPool with count > 1: 30 threads × 50 ops concurrent writes, zero corruption errors
  - Submit-after-destroy (standalone client): throws exception, does not hang
  - Submit-after-destroy (MultiplexPool): handles gracefully (auto-reconnect or throw), does not hang
  - Rapid submit-after-destroy: 20 concurrent submits after destroy all throw, none hang
- Existing cluster ConcurrencyTests (50 threads SET/GET storm, topology refresh, node failure) pass with multiplexing enabled by default (verified via make test)
- Registered `StandaloneConcurrencyTests` in `redis-client.cabal` and `test/LibraryE2E.hs`
- All 45 E2E tests pass (39 existing + 6 new) in 324s
- All unit tests pass (RespSpec 46, MultiplexerSpec 14, MultiplexPoolSpec 8, ClusterSpec 14, ClusterCommandSpec 40, FillHelpersSpec 27)
- Files changed:
  - `test/LibraryE2E/StandaloneConcurrencyTests.hs` (new — 203 lines)
  - `test/LibraryE2E.hs` (modified — added StandaloneConcurrencyTests import and spec call)
  - `redis-client.cabal` (modified — added StandaloneConcurrencyTests to other-modules)
- **Learnings for future iterations:**
  - MultiplexPool auto-reconnects dead multiplexers — closeMultiplexPool + submitToNode may succeed (new mux created), not necessarily throw
  - For standalone concurrency tests, use a single shared client across all threads (tests the multiplexer's thread safety)
  - The `try` pattern is essential for submit-after-destroy tests — it proves the operation completed (didn't hang)
  - Standalone Redis container (`redis-standalone.local:6390`) is sufficient for all MultiplexPool tests since MultiplexPool is node-agnostic
---

## 2026-02-13 - US-008 (multiplexer-default branch)
- Updated README.md with Library Usage section containing standalone multiplexed client and cluster client usage examples
- Updated README.md project structure to mention multiplexer and standalone client in lib/cluster/
- Updated README.md unit test section to include MultiplexerSpec and MultiplexPoolSpec
- Added CHANGELOG.md v0.6.0.0 entry documenting: multiplexing-by-default, standalone client, new test suites
- Verified all cabal exports (exposed-modules, reexported-modules) and Redis.hs re-exports already in place from US-001/US-002
- Typecheck passes, all unit tests pass (RespSpec, ClusterSpec, ClusterCommandSpec, MultiplexerSpec, MultiplexPoolSpec)
- Files changed:
  - `README.md` (modified — added Library Usage section, updated project structure, updated test commands)
  - `CHANGELOG.md` (modified — added v0.6.0.0 entry)
  - `prd.json` (updated US-008 to passes: true)
- **Learnings for future iterations:**
  - Documentation stories are quick when prior stories already set up exports/re-exports correctly
  - CHANGELOG version bump should reflect the scope of all stories in the PRD, not just the doc story
  - README examples should match the haddock examples in the module source for consistency
---
